{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Recommender_System_MF_GMF_MLP_Neumf_ALS.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hungtangwei/Recommender_System_MF_GMF_MLP_Neumf_ALS/blob/master/Recommender_System_MF_GMF_MLP_Neumf_ALS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94cqr-pzOMR-",
        "colab_type": "text"
      },
      "source": [
        "# Talbe of contents\n",
        "\n",
        "* [Task 1:  Recommender System Challenge](#Task1)\n",
        "    * [Introduction](#introduction1)\n",
        "    * [Import libraries](#libraries1)\n",
        "    * [Read the data set](#readdata1)\n",
        "    * [Alternating Least Squares Model](#als)\n",
        "    * [Matrix factorization Model (With Bias)](#mf)\n",
        "    * [Neural matrix factorization Model](#neumf)\n",
        "    * [Compare model and result conclusion](#conclusion1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KANLHqKfOMSA",
        "colab_type": "text"
      },
      "source": [
        "## Task 1: Recommender System Challenge \n",
        "<a id=\"Task1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBA4Upz6OMSB",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "<a id=\"introduction1\"></a>\n",
        "In the task 1, we will build a recommender system to recommend a list of items to each user. For this task, the dataset we will use is collected from an online social network platform. This dataset records the information that a set of interactions between users and items. If a user engages with an item, then there will be a record in the dataset.\n",
        "\n",
        "To fininsh this task, we will use three models which are alternating least squares model, matrix factorization mode with bias, and neural matrix factorization model to build the recommender system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc2gI8BFOMSC",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries\n",
        "<a id=\"libraries1\"></a>\n",
        "In this part, we will import some libraries for the following task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTHE_wlcOMSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install implicit\n",
        "import implicit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "import scipy.sparse as sparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SACJSDROMSK",
        "colab_type": "text"
      },
      "source": [
        "### Read the data set\n",
        "<a id=\"readdata1\"></a>\n",
        "In this part, I will read the train, val, and test data set.\n",
        "\n",
        "The data sets I will use are training data which contains a set of interactions between users and items, test data which contains a list with 100 candidates, and validation data which is similar to the test data but with rating.\n",
        "\n",
        "However, the train data set only contains rating of 1. Therefore, I combine the validation data with original train data set to the new train data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7rEa6QxOMSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the train, validation, and test data set\n",
        "df1= pd.read_csv('train_data.csv')\n",
        "val= pd.read_csv('validation_data.csv')\n",
        "test = pd.read_csv(\"test_data.csv\")\n",
        "train=pd.concat([df1,val])\n",
        "train=train.sort_values(by=['user_id'],ascending=[True])\n",
        "\n",
        "#drop the duplicate data\n",
        "train = train.drop_duplicates()\n",
        "val = val.drop_duplicates()\n",
        "test = test.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwbNcR17OMSQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "2b8361d0-50f7-4013-fa21-4b7fc2a519a0"
      },
      "source": [
        "# cacluate the number of users and items\n",
        "num_users = len(train.user_id.unique())\n",
        "num_items = len(train.item_id.unique())\n",
        "print(num_users, num_items) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2239 2174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRr4t2_2OMSW",
        "colab_type": "text"
      },
      "source": [
        "### Alternating Least Squares model\n",
        "<a id=\"als\"></a>\n",
        "\n",
        "The first model I will use is Alternating Least Squares model which is a form of matrix factorization that reduces user-item matrix to a much smaller amount of dimension called latent or hidden features.\n",
        "\n",
        "In this ALS Recommendation Model, I will use the train data set and test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcWR6ClJOMSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert to sparse matrix\n",
        "sparse_content_person = sparse.csr_matrix((train['rating'].astype(float), (train['item_id'], train['user_id'])))\n",
        "sparse_person_content = sparse.csr_matrix((train['rating'].astype(float), (train['user_id'], train['item_id'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju3-nrTuOMSc",
        "colab_type": "code",
        "colab": {
          "referenced_widgets": [
            "82b5405518314e2fa735a32340de63ae"
          ]
        },
        "outputId": "c00c06e1-4d52-47cf-f99b-2d670fd275ac"
      },
      "source": [
        "#fit the model\n",
        "np.random.seed(25)\n",
        "alpha = 15\n",
        "data = (sparse_content_person * alpha).astype('double')\n",
        "model_ALS = implicit.als.AlternatingLeastSquares(factors=7, regularization=0.1, iterations=100,use_gpu=False)\n",
        "model_ALS.fit(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82b5405518314e2fa735a32340de63ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFoyRbjwOMSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set the test user list\n",
        "test_user_list=list(set(test.user_id.tolist()))\n",
        "user_item_dict={}\n",
        "for i in test_user_list:\n",
        "    fliter = (test[\"user_id\"] == i)\n",
        "    user_item_dict[i]=test[fliter].item_id.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djULeDrVOMSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_ALS=[]\n",
        "#get the recommend for test user\n",
        "for i in test_user_list:\n",
        "    user_id=i\n",
        "    recommend_ALS=model_ALS.recommend(userid=i,user_items=sparse_person_content,N=1000)\n",
        "    count_ALS=0\n",
        "    for j in recommend_ALS:\n",
        "        item_id_ALS=j[0]\n",
        "        if item_id_ALS in user_item_dict[i] and count_ALS<10:\n",
        "            count_ALS+=1\n",
        "            final_ALS.append(item_id_ALS)\n",
        "        elif count_ALS > 10:\n",
        "            continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97bcc7S_OMSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#output the csv file\n",
        "final_id=test_user_list*10\n",
        "final_id.sort()\n",
        "df_final_ALS=pd.DataFrame({'user_id':final_id,'item_id':final_ALS})\n",
        "df_final_ALS.to_csv('29375932.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWDQAsvyOMSv",
        "colab_type": "text"
      },
      "source": [
        "### Matrix factorization model (With Bias)\n",
        "<a id=\"mf\"></a>\n",
        "\n",
        "The second model I use is matrix factorization model which is an algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices. Besides, due to variation in rating among different users, I add item bias and user bias to this matrix factorization model.\n",
        "\n",
        "The dataset I used in this model is train, val, and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XY6VffAOMSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a class of matrix factorization model \n",
        "class MF_bias(nn.Module):\n",
        "    def __init__(self, num_users, num_items, emb_size=100):\n",
        "        super(MF_bias, self).__init__()\n",
        "        self.embedding_user = nn.Embedding(num_users, emb_size)\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\n",
        "        self.embedding_item= nn.Embedding(num_items, emb_size)\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\n",
        "        self.embedding_user.weight.data.uniform_(0,0.05)\n",
        "        self.embedding_item.weight.data.uniform_(0,0.05)\n",
        "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
        "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
        "        \n",
        "    def forward(self, u, v):\n",
        "        U = self.embedding_user(u)\n",
        "        V = self.embedding_item(v)\n",
        "        b_u = self.user_bias(u).squeeze()\n",
        "        b_v = self.item_bias(v).squeeze()\n",
        "        rating=(U*V).sum(1) +  b_u  + b_v\n",
        "        return rating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T79K5ORSOMS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "def train_epocs(model, epochs=30, lr=0.01, wd=0.0, unsqueeze=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "        users = torch.LongTensor(train.user_id.values)#.cuda()\n",
        "        items = torch.LongTensor(train.item_id.values)#.cuda()\n",
        "        ratings = torch.FloatTensor(train.rating.values)#.cuda()\n",
        "        if unsqueeze:\n",
        "            ratings = ratings.unsqueeze(1)\n",
        "        y_hat = model(users, items)\n",
        "        loss = F.mse_loss(y_hat, ratings)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(loss.item()) \n",
        "    test_loss(model, unsqueeze)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsHV63N8OMS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_loss(model, unsqueeze=False):\n",
        "    model.eval()\n",
        "    users = torch.LongTensor(val.user_id.values)#.cuda()\n",
        "    items = torch.LongTensor(val.item_id.values)#.cuda()\n",
        "    ratings = torch.FloatTensor(val.rating.values)#.cuda()\n",
        "    if unsqueeze:\n",
        "        ratings = ratings.unsqueeze(1)\n",
        "    y_hat = model(users, items)\n",
        "    loss = F.mse_loss(y_hat, ratings)\n",
        "    print(\"test loss %.3f \" % loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_MolGZ3OMS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the moedle\n",
        "model_mf = MF_bias(num_users, num_items, emb_size=100)#.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYPgxzLDOMTC",
        "colab_type": "code",
        "colab": {},
        "outputId": "cf73851c-8563-4ac9-f02d-94c3a5ee6f57"
      },
      "source": [
        "# train the modle\n",
        "#train_epocs(model_mf, epochs=1700, lr=0.0001, wd=1e-5) #if use cuda, the epochs could use 1700\n",
        "train_epocs(model_mf, epochs=20, lr=0.0001, wd=1e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.11249944567680359\n",
            "0.11241340637207031\n",
            "0.11232789605855942\n",
            "0.11224278062582016\n",
            "0.11215800791978836\n",
            "0.11207377910614014\n",
            "0.11199000477790833\n",
            "0.11190664768218994\n",
            "0.11182374507188797\n",
            "0.1117413341999054\n",
            "0.1116594672203064\n",
            "0.11157800257205963\n",
            "0.11149706691503525\n",
            "0.11141666024923325\n",
            "0.11133672297000885\n",
            "0.11125726997852325\n",
            "0.11117841303348541\n",
            "0.1110999658703804\n",
            "0.11102209240198135\n",
            "0.1109447181224823\n",
            "test loss 0.014 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aNudCiVOMTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predit the rating and output the result to csv file\n",
        "model_mf.eval()\n",
        "with torch.no_grad():\n",
        "    test_users = torch.LongTensor(test.user_id.values)#.cuda()\n",
        "    test_items = torch.LongTensor(test.item_id.values)#.cuda()\n",
        "    y_hat_mf = model_mf(test_users, test_items)\n",
        "    rating_mf = [element.item() for element in y_hat_mf.flatten()]\n",
        "    df_mf=pd.DataFrame({'user_id':test.user_id,'item_id':test.item_id,'rating':rating_mf})\n",
        "    df_mf_sort=df_mf.sort_values(by=['user_id','rating'],ascending=[True,False])\n",
        "    final_mf=df_mf_sort.groupby('user_id').head(10)\n",
        "    final_mf=final_mf.drop(columns=['rating'])\n",
        "    #final_mf.to_csv('final_mf.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpmVlHIHOMTM",
        "colab_type": "text"
      },
      "source": [
        "### Neural matrix factorization model\n",
        "<a id=\"neumf\"></a>\n",
        "\n",
        "The final model is neural matrix factorization model which is the fusion of generalized matrix factorization and multi-layer perceptronto reinforce each other to better model the complex user-iterm interactions. \n",
        "\n",
        "In addition, I will pretrain the generalized matrix factorization model(GMF) and multi-layer perceptron model(MLP) and then load the wieght from GMF model and MLP model for initializing NerMF model.\n",
        "\n",
        "The dataset I used in this model is train, val, and test datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu0UUAXtOMTM",
        "colab_type": "text"
      },
      "source": [
        "#### Generalized Matrix Factorization model (Pre-train model)\n",
        "\n",
        "In this model, I will implement a generalized version of MF under neural collaborative filtering that uses the sigmoid function to output. Inaddition, I add the user biases and item biases to this model. Finally, I will save the weight of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gdJcCt8OMTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "# define the class of GMF model\n",
        "class GMF(torch.nn.Module):\n",
        "    def __init__(self, num_users,num_items,latent_dim=100):\n",
        "        super(GMF, self).__init__()\n",
        "        self.embedding_user = torch.nn.Embedding(num_embeddings=num_users, embedding_dim=latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(num_embeddings=num_items, embedding_dim=latent_dim)\n",
        "        # add the user and item bias\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\n",
        "        #uniform the embedding of user and item \n",
        "        self.embedding_user.weight.data.uniform_(0,0.05)\n",
        "        self.embedding_item.weight.data.uniform_(0,0.05)\n",
        "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
        "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
        "        self.affine_output = torch.nn.Linear(in_features=latent_dim, out_features=1)\n",
        "        # set sigmoid function\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        item_bias_mf = self.item_bias(item_indices)\n",
        "        user_bias_mf = self.user_bias(user_indices)\n",
        "        element_product = torch.mul(user_embedding, item_embedding)\n",
        "        element_product = element_product + item_bias_mf+user_bias_mf\n",
        "        logits = self.affine_output(element_product)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GkY_GixOMTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the function of training model\n",
        "def train_epocs_gmf(model, epochs=10, lr=1e-3, wd=0.0, unsqueeze=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "        users = torch.LongTensor(train.user_id.values)#.cuda()\n",
        "        items = torch.LongTensor(train.item_id.values)#.cuda()\n",
        "        ratings = torch.FloatTensor(train.rating.values)#.cuda()\n",
        "        if unsqueeze:\n",
        "            ratings = ratings.unsqueeze(1)\n",
        "        y_hat = model(users, items)\n",
        "        loss = F.mse_loss(y_hat, ratings)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(loss.item()) \n",
        "    test_loss(model, unsqueeze)\n",
        "    print('save')\n",
        "    # save the weight of model\n",
        "    torch.save(model.state_dict(), 'gmf_model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VQ0p1NjOMTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the gmf model\n",
        "gmf_model = GMF(num_users, num_items)#.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9tw7auaOMTb",
        "colab_type": "code",
        "colab": {},
        "outputId": "cb5fe441-1319-47fc-fbdc-b61b87cc0f22"
      },
      "source": [
        "# train the gmf model\n",
        "#train_epocs_gmf(gmf_model, epochs=1500, lr=0.0001, wd=1e-5, unsqueeze=True) #if use cuda, the epochs could use 1500\n",
        "train_epocs_gmf(gmf_model, epochs=20, lr=0.0001, wd=1e-5, unsqueeze=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2544608414173126\n",
            "0.2544329762458801\n",
            "0.2544041872024536\n",
            "0.25437474250793457\n",
            "0.254344642162323\n",
            "0.2543134093284607\n",
            "0.2542816698551178\n",
            "0.25424909591674805\n",
            "0.25421568751335144\n",
            "0.254181444644928\n",
            "0.2541462481021881\n",
            "0.25411009788513184\n",
            "0.2540731430053711\n",
            "0.2540351450443268\n",
            "0.25399625301361084\n",
            "0.2539563477039337\n",
            "0.2539154887199402\n",
            "0.25387343764305115\n",
            "0.2538304030895233\n",
            "0.2537863552570343\n",
            "test loss 0.255 \n",
            "save\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgwZoDv6OMTf",
        "colab_type": "text"
      },
      "source": [
        "#### Multi-Layer Perceptron Model (Pre-train model)\n",
        "\n",
        "In this model, we add hidden layers on the concatenated vector, using a standard multi-layer perceptron to learn the interaction between user and item latent features. Besides, I also add the user biases and item biases to this model. Finally, I will save the weight of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8vHmNN6OMTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the class of MLP model\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, num_users,num_items,latent_dim=32,layers=[64,32,16,8]):\n",
        "        super(MLP, self).__init__()\n",
        "        self.embedding_user = torch.nn.Embedding(num_embeddings=num_users, embedding_dim=latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(num_embeddings=num_items, embedding_dim=latent_dim)\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\n",
        "        self.embedding_user.weight.data.uniform_(0,0.05)\n",
        "        self.embedding_item.weight.data.uniform_(0,0.05)\n",
        "        self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
        "        self.item_bias.weight.data.uniform_(-0.01,0.01)\n",
        "\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(layers[:-1], layers[1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "        self.affine_output = torch.nn.Linear(in_features=layers[-1], out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        user_embedding = F.dropout(user_embedding, 0.1)\n",
        "        item_embedding = F.dropout(item_embedding, 0.1)\n",
        "        item_bias_mlp = self.item_bias(item_indices)\n",
        "        user_bias_mlp = self.user_bias(user_indices)\n",
        "        vector = torch.cat([user_embedding, item_embedding + item_bias_mlp +  user_bias_mlp], dim=-1)\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            vector = self.fc_layers[idx](vector)\n",
        "            vector = torch.nn.ReLU()(vector)\n",
        "            vector = F.dropout(vector, 0.1)\n",
        "\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Oe93to-OMTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the train function\n",
        "def train_epocs_mlp(model, epochs=10, lr=0.01, wd=0.0, unsqueeze=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "        users = torch.LongTensor(train.user_id.values)#.cuda()\n",
        "        items = torch.LongTensor(train.item_id.values)#.cuda()\n",
        "        ratings = torch.FloatTensor(train.rating.values)#.cuda()\n",
        "        if unsqueeze:\n",
        "            ratings = ratings.unsqueeze(1)\n",
        "        y_hat = model(users, items)\n",
        "        loss = F.mse_loss(y_hat, ratings)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(loss.item()) \n",
        "    test_loss(model, unsqueeze)\n",
        "    print('save')\n",
        "    # save the model\n",
        "    torch.save(model.state_dict(), 'mlp_model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuaVOs_tOMTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the mlp model\n",
        "mlp_model = MLP(num_users, num_items)#.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl67BHIeOMTr",
        "colab_type": "code",
        "colab": {},
        "outputId": "40c3a100-3413-4136-b4a2-23c30d4259f9"
      },
      "source": [
        "# train the model\n",
        "#train_epocs_mlp(mlp_model, epochs=1000, lr=0.001, wd=1e-6, unsqueeze=True) #if use cuda the epochs could use 1000\n",
        "train_epocs_mlp(mlp_model, epochs=20, lr=0.001, wd=1e-6, unsqueeze=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.27081772685050964\n",
            "0.2700363099575043\n",
            "0.2692871391773224\n",
            "0.2685478627681732\n",
            "0.2678098976612091\n",
            "0.26705998182296753\n",
            "0.26635944843292236\n",
            "0.2656811773777008\n",
            "0.26500770449638367\n",
            "0.2643532454967499\n",
            "0.26372426748275757\n",
            "0.26307597756385803\n",
            "0.2623986601829529\n",
            "0.2617660164833069\n",
            "0.26110222935676575\n",
            "0.2604369521141052\n",
            "0.25979703664779663\n",
            "0.2591019570827484\n",
            "0.25841549038887024\n",
            "0.2577289938926697\n",
            "test loss 0.259 \n",
            "save\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJcP7ItdOMTu",
        "colab_type": "text"
      },
      "source": [
        "#### Neural matrix factorization Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1hHHv4cOMTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuMF(torch.nn.Module):\n",
        "    def __init__(self, num_users, num_items, latent_dim_mf=100, latent_dim_mlp=32, layers=[64,32,16,8]):\n",
        "        super(NeuMF, self).__init__()\n",
        "\n",
        "        # Part of GMF\n",
        "        self.embedding_user_mf = torch.nn.Embedding(num_embeddings=num_users, embedding_dim=latent_dim_mf)\n",
        "        self.embedding_item_mf = torch.nn.Embedding(num_embeddings=num_items, embedding_dim=latent_dim_mf)\n",
        "        self.embedding_user_mf_bias = nn.Embedding(num_users, 1)\n",
        "        self.embedding_item_mf_bias = nn.Embedding(num_items, 1)\n",
        "        self.embedding_user_mf_bias.weight.data.uniform_(-0.01,0.01)\n",
        "        self.embedding_item_mf_bias.weight.data.uniform_(-0.01,0.01)\n",
        "\n",
        "        # Part of MLP\n",
        "        self.embedding_user_mlp = torch.nn.Embedding(num_embeddings=num_users, embedding_dim=latent_dim_mlp)\n",
        "        self.embedding_item_mlp = torch.nn.Embedding(num_embeddings=num_items, embedding_dim=latent_dim_mlp)\n",
        "        self.embedding_user_mlp_bias = nn.Embedding(num_users, 1)\n",
        "        self.embedding_item_mlp_bias = nn.Embedding(num_items, 1)\n",
        "        self.embedding_user_mlp_bias.weight.data.uniform_(-0.01,0.01)\n",
        "        self.embedding_item_mlp_bias.weight.data.uniform_(-0.01,0.01)\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(layers[:-1], layers[1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=layers[-1] + latent_dim_mf, out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "\n",
        "        # Part of GMF\n",
        "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
        "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
        "        user_embedding_mf = F.dropout(user_embedding_mf, 0.1)\n",
        "        item_embedding_mf = F.dropout(item_embedding_mf, 0.1)\n",
        "        item_bias_mf = self.embedding_item_mf_bias(item_indices)\n",
        "        user_bias_mf = self.embedding_user_mf_bias(user_indices)\n",
        "        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
        "        mf_vector = mf_vector + item_bias_mf+user_bias_mf\n",
        "        \n",
        "        # Part of MLP\n",
        "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
        "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
        "        user_embedding_mlp = F.dropout(user_embedding_mlp, 0.1)\n",
        "        item_embedding_mlp = F.dropout(item_embedding_mlp, 0.1)\n",
        "        item_bias_mlp = self.embedding_item_mlp_bias(item_indices)\n",
        "        user_bias_mlp = self.embedding_user_mlp_bias(user_indices)\n",
        "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp + item_bias_mlp +  user_bias_mlp], dim=-1)\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
        "            mlp_vector = torch.nn.ReLU()(mlp_vector)\n",
        "            mlp_vector = F.dropout(mlp_vector, 0.1)\n",
        "\n",
        "        # Fusion of GMF and MLP\n",
        "        vector = torch.cat([F.dropout(mlp_vector,0.1), F.dropout(mf_vector,0.1)], dim=-1)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    # load the wieght from GMF model and MLP model\n",
        "    def load_pretrain_weights(self):\n",
        "        \n",
        "        # Part of GMF\n",
        "        gmf_model = GMF(num_users,num_items,latent_dim=100)#.cuda()\n",
        "        gmf_model.load_state_dict(torch.load('gmf_model.pt'))\n",
        "        self.embedding_user_mf.weight.data = gmf_model.embedding_user.weight.data\n",
        "        self.embedding_item_mf.weight.data = gmf_model.embedding_item.weight.data\n",
        "\n",
        "        # Part of MLP\n",
        "        mlp_model = MLP(num_users,num_items,latent_dim=32,layers=[64,32,16,8])#.cuda()\n",
        "        mlp_model.load_state_dict(torch.load('mlp_model.pt'))\n",
        "        self.embedding_user_mlp.weight.data = mlp_model.embedding_user.weight.data\n",
        "        self.embedding_item_mlp.weight.data = mlp_model.embedding_item.weight.data        \n",
        "        for idx in range(len(self.fc_layers)):\n",
        "            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n",
        "        \n",
        "        # Concatenate weights of the two models.\n",
        "        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n",
        "        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCVqyeCVOMT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the train function\n",
        "def train_epocs_NeuMF(model, epochs=10, lr=0.01, wd=0.0, unsqueeze=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    model.train()\n",
        "    for i in range(epochs):\n",
        "        users = torch.LongTensor(train.user_id.values)#.cuda()\n",
        "        items = torch.LongTensor(train.item_id.values)#.cuda()\n",
        "        ratings = torch.FloatTensor(train.rating.values)#.cuda()\n",
        "        if unsqueeze:\n",
        "            ratings = ratings.unsqueeze(1)\n",
        "        y_hat = model(users, items)\n",
        "        loss = F.mse_loss(y_hat, ratings)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(loss.item()) \n",
        "    test_loss(model, unsqueeze)\n",
        "    print('save')\n",
        "    torch.save(NeuMF_model.state_dict(), 'NeuMF_model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4UVXjdROMT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the NeuMF model\n",
        "NeuMF_model = NeuMF(num_users, num_items,layers=[64,32,16,8])#.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMkc6Z2NOMT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the pre-training models of GMF and MLP\n",
        "NeuMF_model.load_pretrain_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9uN0JayOMUA",
        "colab_type": "code",
        "colab": {},
        "outputId": "9822eb6a-36c0-48f3-ba01-a84af5644d88"
      },
      "source": [
        "# train the model\n",
        "#train_epocs_NeuMF(NeuMF_model, epochs=1500, lr=0.0001, wd=1e-6, unsqueeze=True) #if use cuda, the epochs could use 1500\n",
        "train_epocs_NeuMF(NeuMF_model, epochs=20, lr=0.0001, wd=1e-6, unsqueeze=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2685481607913971\n",
            "0.26849186420440674\n",
            "0.26842349767684937\n",
            "0.26835474371910095\n",
            "0.2683005630970001\n",
            "0.2682367265224457\n",
            "0.2681454122066498\n",
            "0.26808658242225647\n",
            "0.2680085599422455\n",
            "0.2679571211338043\n",
            "0.26787233352661133\n",
            "0.26780810952186584\n",
            "0.26772287487983704\n",
            "0.267640084028244\n",
            "0.26756560802459717\n",
            "0.26749274134635925\n",
            "0.26740965247154236\n",
            "0.2673214375972748\n",
            "0.2672480344772339\n",
            "0.26716357469558716\n",
            "test loss 0.272 \n",
            "save\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYYHDuPjOMUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the predicted rating and save the ouput to csv file\n",
        "NeuMF_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_users = torch.LongTensor(test.user_id.values)#.cuda()\n",
        "    test_items = torch.LongTensor(test.item_id.values)#.cuda()\n",
        "    y_hat_NeuMF = NeuMF_model(test_users, test_items)\n",
        "    rating_NeuMF = [element.item() for element in y_hat_NeuMF.flatten()]\n",
        "    df_NeuMF=pd.DataFrame({'user_id':test.user_id,'item_id':test.item_id,'rating':rating_NeuMF})\n",
        "    df_NeuMF_sort=df_NeuMF.sort_values(by=['user_id','rating'],ascending=[True,False])\n",
        "    final_NeuMF=df_NeuMF_sort.groupby('user_id').head(10)\n",
        "    final_NeuMF=final_NeuMF.drop(columns=['rating'])\n",
        "    #final_NeuMF.to_csv('final_NeuMF.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIPtBIsxOMUM",
        "colab_type": "text"
      },
      "source": [
        "### Compare model and result conclusion\n",
        "<a id=\"conclusion1\"></a>\n",
        "\n",
        "The final  Normalized Discounted Cumulative Gain (NDGC) for these thress model are:\n",
        "* Alternating Least Squares model: 0.21\n",
        "* Matrix factorization model (With Bias): 0.15\n",
        "* Neural matrix factorization model:0.14\n",
        "\n",
        "According to NDGC, we could notice that the Alternating Least Squares model has highest Normalized Discountd Coumulatice Gain. This means the ALS model is the most suitable model in this case. The reason may because in this case, the rating are only 0 and 1, and it has small data size. Therefore, if we use complex model to build the model, the complex model will not perform well. Finally, this is the reason I choice the ALS model as my model to submit to Kaggle."
      ]
    }
  ]
}